{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho de Visão Computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import glob as gb\n",
    "import os\n",
    "from importlib import reload\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from models.alexNet import MY_AlexNet\n",
    "from models.vgg16 import MY_VGG16\n",
    "from utils.utils import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração dos Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data_C10 = datasets.CIFAR10(\n",
    "    root=\"../data/datasets/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data_C10 = datasets.CIFAR10(\n",
    "    root=\"../data/datasets/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ../data/datasets/\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ../data/datasets/\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_C10, test_data_C10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building-vs-forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"../data/datasets/buildings-vs-forests/test_set.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../data/datasets/buildings-vs-forests/\")\n",
    "    \n",
    "with ZipFile(\"../data/datasets/buildings-vs-forests/traning_set.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../data/datasets/buildings-vs-forests/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buildings': 0, 'forest': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trian_path = '../data/datasets/buildings-vs-forests/traning_set'\n",
    "test_path = '../data/datasets/buildings-vs-forests/test_set'\n",
    "\n",
    "training_data_BF = ImageFolder(trian_path, transform = tfm)\n",
    "test_data_BF = ImageFolder(test_path, transform = tfm)\n",
    "\n",
    "len_train = len(training_data_BF)\n",
    "len_test = len(test_data_BF)\n",
    "\n",
    "training_data_BF.class_to_idx\n",
    "test_data_BF.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação - CIFAR10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Epócas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [22:02<00:00, 330.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5 Epochs</th>\n",
       "      <th>10 Epochs</th>\n",
       "      <th>20 Epochs</th>\n",
       "      <th>50 Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5 Epochs  10 Epochs  20 Epochs  50 Epochs\n",
       "0       1.0        0.0      0.132      0.140\n",
       "1       0.0        1.0      0.000      0.304\n",
       "2       0.0        0.0      0.000      0.169\n",
       "3       0.0        0.0      0.000      0.000\n",
       "4       0.0        0.0      0.293      0.002\n",
       "5       0.0        0.0      0.104      0.001\n",
       "6       0.0        0.0      0.000      0.000\n",
       "7       0.0        0.0      0.000      0.000\n",
       "8       0.0        0.0      0.000      0.220\n",
       "9       0.0        0.0      0.934      0.783"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = [5, 10, 20, 50]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(epoch):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "    \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'5 Epochs': accuracys[0], '10 Epochs': accuracys[1], '20 Epochs': accuracys[2], '50 Epochs': accuracys[3]})\n",
    "df.to_csv('../data/results/cifar10_alexNet_epochs.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [31:12<00:00, 624.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size: 64</th>\n",
       "      <th>Batch Size: 128</th>\n",
       "      <th>Batch Size: 256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.562</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.529</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Size: 64  Batch Size: 128  Batch Size: 256\n",
       "0           0.796            0.000            0.000\n",
       "1           0.000            0.000            0.000\n",
       "2           0.002            0.000            0.000\n",
       "3           0.000            0.000            0.000\n",
       "4           0.039            0.009            0.000\n",
       "5           0.000            0.000            0.002\n",
       "6           0.000            0.000            0.000\n",
       "7           0.000            0.000            0.000\n",
       "8           0.005            0.562            1.000\n",
       "9           0.529            0.731            0.000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "        \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Batch Size: 64': accuracys[0], 'Batch Size: 128': accuracys[1], 'Batch Size: 256': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_alexNet_batchSize.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Taxa de Aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [38:12<00:00, 764.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate: 1e-2</th>\n",
       "      <th>Learning Rate: 1e-3</th>\n",
       "      <th>Learning Rate: 1e-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.785</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.767</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate: 1e-2  Learning Rate: 1e-3  Learning Rate: 1e-4\n",
       "0                0.706                0.978                0.009\n",
       "1                0.785                0.000                0.000\n",
       "2                0.559                0.016                0.000\n",
       "3                0.622                0.000                0.000\n",
       "4                0.549                0.000                0.000\n",
       "5                0.577                0.000                0.000\n",
       "6                0.767                0.400                0.999\n",
       "7                0.684                0.000                0.000\n",
       "8                0.840                0.002                0.000\n",
       "9                0.732                0.000                0.000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnings = [1e-2, 1e-3, 1e-4]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for rate in tqdm(learnings):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=64)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=64)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Learning Rate: 1e-2': accuracys[0], 'Learning Rate: 1e-3': accuracys[1], 'Learning Rate: 1e-4': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_alexNet_learnRate.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avaliando Melhor Configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:48<00:00, 796.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter 1</th>\n",
       "      <th>Iter 2</th>\n",
       "      <th>Iter 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.679</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.701</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.742</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.767</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iter 1  Iter 2  Iter 3\n",
       "0   0.679   0.724   0.679\n",
       "1   0.761   0.803   0.799\n",
       "2   0.589   0.367   0.523\n",
       "3   0.406   0.331   0.627\n",
       "4   0.756   0.750   0.614\n",
       "5   0.565   0.548   0.450\n",
       "6   0.701   0.748   0.739\n",
       "7   0.742   0.815   0.687\n",
       "8   0.747   0.690   0.848\n",
       "9   0.767   0.714   0.800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning = 1e-2\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Iter 1': accuracys[0], 'Iter 2': accuracys[1], 'Iter 3': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_alexNet_bestConfig.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [1:05:47<00:00, 986.94s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5 Epochs</th>\n",
       "      <th>10 Epochs</th>\n",
       "      <th>20 Epochs</th>\n",
       "      <th>50 Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5 Epochs  10 Epochs  20 Epochs  50 Epochs\n",
       "0       0.0        0.0        0.0        0.0\n",
       "1       1.0        1.0        0.0        1.0\n",
       "2       0.0        0.0        1.0        0.0\n",
       "3       0.0        0.0        0.0        0.0\n",
       "4       0.0        0.0        0.0        0.0\n",
       "5       0.0        0.0        0.0        0.0\n",
       "6       0.0        0.0        0.0        0.0\n",
       "7       0.0        0.0        0.0        0.0\n",
       "8       0.0        0.0        0.0        0.0\n",
       "9       0.0        0.0        0.0        0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = [5, 10, 20, 50]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = []\n",
    "error = 999\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(epoch):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "    \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'5 Epochs': accuracys[0], '10 Epochs': accuracys[1], '20 Epochs': accuracys[2], '50 Epochs': accuracys[3]})\n",
    "df.to_csv('../data/results/cifar10_vgg16_epochs.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [24:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projetos\\Python\\UFJF\\8 Periodo\\Trabalho_vc_NEO\\src\\main.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projetos/Python/UFJF/8%20Periodo/Trabalho_vc_NEO/src/main.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projetos/Python/UFJF/8%20Periodo/Trabalho_vc_NEO/src/main.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projetos/Python/UFJF/8%20Periodo/Trabalho_vc_NEO/src/main.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train(train_dataloader, model, loss_fn, optimizer, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projetos/Python/UFJF/8%20Periodo/Trabalho_vc_NEO/src/main.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     accuracy, error \u001b[39m=\u001b[39m test_for_class(test_dataloader, model, loss_fn, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projetos/Python/UFJF/8%20Periodo/Trabalho_vc_NEO/src/main.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m accuracys\u001b[39m.\u001b[39mappend(accuracy)\n",
      "File \u001b[1;32mc:\\Projetos\\Python\\UFJF\\8 Periodo\\Trabalho_vc_NEO\\src\\utils\\utils.py:20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m totalLoss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Itera sobre os lotes\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# transforma as entradas no formato do dispositivo utilizado (CPU ou GPU)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Faz a predição para os valores atuais dos parâmetros\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], F_pil\u001b[39m.\u001b[39;49mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "        \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Batch Size: 64': accuracys[0], 'Batch Size: 128': accuracys[1], 'Batch Size: 256': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_vgg16_batchSize.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Taxa de Aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnings = [1e-2, 1e-3, 1e-4]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for rate in tqdm(learnings):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=64)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=64)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Learning Rate: 1e-2': accuracys[0], 'Learning Rate: 1e-3': accuracys[1], 'Learning Rate: 1e-4': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_vgg16_learnRate.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avaliando Melhor Configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning = 1e-2\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test_for_class(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Iter 1': accuracys[0], 'Iter 2': accuracys[1], 'Iter 3': accuracys[2]})\n",
    "df.to_csv('../data/results/cifar10_vgg16_bestConfig.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação - Building-vs-Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [5, 10, 20, 50]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data_BF, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_BF, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 2)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(epoch):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "    \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Epochs': epochs, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_alexNet_epochs.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "        \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Batch Size': batch_sizes, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_alexNet_batchSize.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Taxa de Aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnings = [1e-2, 1e-3, 1e-4]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for rate in tqdm(learnings):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=64)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=64)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Learning Rate': learnings, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_alexNet_learnRate.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avaliando Melhor Configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning = 1e-2\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_AlexNet(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_alexNet_bestConfig.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [5, 10, 20, 50]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(training_data_BF, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_BF, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 2)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(epoch):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "    \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Epochs': epochs, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_vgg16_epochs.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "        \n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Batch Size': batch_sizes, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_vgg16_batchSize.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variando Taxa de Aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnings = [1e-2, 1e-3, 1e-4]\n",
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "for rate in tqdm(learnings):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=64)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=64)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Learning Rate': learnings, 'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_vgg16_learnRate.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avaliando Melhor Configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys = []\n",
    "errors = []\n",
    "\n",
    "accuracy = 0\n",
    "error = 999\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning = 1e-2\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    train_dataloader = DataLoader(training_data_C10, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data_C10, batch_size=batch_size)\n",
    "\n",
    "    model = MY_VGG16(n_classes = 10)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(50):\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        accuracy, error = test(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "    accuracys.append(accuracy)\n",
    "    errors.append(error)\n",
    "\n",
    "df = pd.DataFrame({'Accuracy': accuracys, 'Error': errors})\n",
    "df.to_csv('../data/results/BF_vgg16_bestConfig.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
